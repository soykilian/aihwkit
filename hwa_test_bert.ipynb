{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22eb9d0",
   "metadata": {},
   "source": [
    "# BERT Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6b79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Imports from PyTorch.\n",
    "from torch import nn, Tensor, device, no_grad, manual_seed\n",
    "from torch import max as torch_max\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Imports from aihwkit.\n",
    "from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n",
    "import torch \n",
    "from aihwkit.optim import AnalogSGD\n",
    "from aihwkit.simulator.configs import MappingParameter\n",
    "from aihwkit.simulator.rpu_base import cuda\n",
    "from datetime import datetime\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict, defaultdict\n",
    "from numpy import log10, logspace, argsort\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DefaultDataCollator,\n",
    ")\n",
    "from aihwkit.simulator.configs import (\n",
    "    InferenceRPUConfig,\n",
    "    NoiseManagementType,\n",
    "    BoundManagementType,\n",
    ")\n",
    "from aihwkit.simulator.parameters.io import IOParametersIRDropT\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    WeightModifierType,\n",
    "    BoundManagementType,\n",
    "    WeightClipType,\n",
    "    NoiseManagementType,\n",
    "    WeightRemapType,\n",
    ")\n",
    "\n",
    "from torch import save as torch_save, load as torch_load\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "\n",
    "from aihwkit.simulator.configs import (\n",
    "    InferenceRPUConfig,\n",
    "    WeightModifierType, WeightClipType,\n",
    "    WeightNoiseType,\n",
    "    BoundManagementType,\n",
    "    NoiseManagementType,\n",
    "    WeightClipParameter,\n",
    "    WeightModifierParameter,\n",
    "    MappingParameter,\n",
    ")\n",
    "\n",
    "from aihwkit.simulator.presets import PresetIOParameters\n",
    "from aihwkit.inference import PCMLikeNoiseModel, GlobalDriftCompensation, ReRamCMONoiseModel\n",
    "from aihwkit.nn.conversion import convert_to_analog\n",
    "from aihwkit.optim import AnalogSGD\n",
    "#import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e07cdbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_rpu_config(modifier_noise, tile_size=512, dac_res=-1, adc_res=-1):\n",
    "    \"\"\"Create RPU Config emulated typical PCM Device\"\"\"\n",
    "\n",
    "    rpu_config = InferenceRPUConfig(\n",
    "        clip=WeightClipParameter(type=WeightClipType.FIXED_VALUE, fixed_value=1.0),\n",
    "        modifier=WeightModifierParameter(\n",
    "            rel_to_actual_wmax=True, type=WeightModifierType.ADD_NORMAL, std_dev=modifier_noise\n",
    "        ),\n",
    "        mapping=MappingParameter(\n",
    "            digital_bias=True,\n",
    "            learn_out_scaling=True,\n",
    "            weight_scaling_omega=1.0,\n",
    "            out_scaling_columnwise=True,\n",
    "            weight_scaling_columnwise=True,\n",
    "            max_input_size=tile_size,\n",
    "            max_output_size=0,\n",
    "        ),\n",
    "        forward=PresetIOParameters(is_perfect=True),\n",
    "        noise_model=ReRamCMONoiseModel(g_max=90, g_min=10,\n",
    "                                                        acceptance_range=0.2,\n",
    "                                                        resistor_compensator=1.23,\n",
    "                                                        single_device=True, \n",
    "                                                        drift_scale=1.0,\n",
    "                                                        read_noise_scale=0.0,\n",
    "                                                        prog_noise_scale=1.0),\n",
    "        drift_compensation=None,\n",
    "    )\n",
    "    return rpu_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95ec6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"csarron/bert-base-uncased-squad-v1\"\n",
    "def create_model(rpu_config):\n",
    "    \"\"\"Return Question Answering model and whether or not it was loaded from a checkpoint\"\"\"\n",
    "\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "    model = convert_to_analog(model, rpu_config)\n",
    "    #model.load_state_dict(torch.load(\"/u/mvc/bert_hwa2tpth\"))\n",
    "    model.remap_analog_weights()\n",
    "    #model.load_state_dict(torch.load(\"/u/mvc/saved_chkpt.pth\", map_location='cuda'))\n",
    "\n",
    "    #print(model)\n",
    "    return model\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LENGTH = 320\n",
    "DOC_STRIDE = 128\n",
    "def preprocess_validation(dataset):\n",
    "    \"\"\"Preprocess the validation set\"\"\"\n",
    "    # Some of the questions have lots of whitespace on the left,\n",
    "    # which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space).\n",
    "    # So we remove that\n",
    "    # left whitespace\n",
    "    dataset[\"question\"] = [q.lstrip() for q in dataset[\"question\"]]\n",
    "\n",
    "    # Tokenize our dataset with truncation and maybe padding,\n",
    "    # but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long,\n",
    "    # each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_dataset = TOKENIZER(\n",
    "        dataset[\"question\"],\n",
    "        dataset[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context,\n",
    "    # we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_dataset.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_dataset[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_dataset[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example\n",
    "        # (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_dataset.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        # One example can give several spans,\n",
    "        # this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_dataset[\"example_id\"].append(dataset[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not\n",
    "        # part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_dataset[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_dataset[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29783a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions(\n",
    "    examples, features, raw_predictions, n_best_size=20, max_answer_length=30\n",
    "):\n",
    "    \"\"\"Postprocess raw predictions\"\"\"\n",
    "    features.set_format(type=features.format[\"type\"], columns=list(features.features.keys()))\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "\n",
    "    # Map examples ids to index\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "\n",
    "    # Create dict of lists, mapping example indices with corresponding feature indices\n",
    "    features_per_example = defaultdict(list)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        # For each example, take example_id, map to corresponding index\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill\n",
    "    predictions = OrderedDict()\n",
    "\n",
    "    print(\n",
    "        f\"Post-processing {len(examples)} example predictions \"\n",
    "        f\"split into {len(features)} features.\"\n",
    "    )\n",
    "\n",
    "    # Loop over all examples\n",
    "    for example_index, example in enumerate(examples):\n",
    "        # Find the feature indices corresponding to the current example\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        # Store valid answers\n",
    "        valid_answers = []\n",
    "\n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "\n",
    "            # This is what will allow us to map some the positions in our\n",
    "            # logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are\n",
    "                    # out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    # Don't consider answers with a length\n",
    "                    # that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    # Map the start token to the index of the start of that token in the context\n",
    "                    # Map the end token to the index of the end of that token in the context\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "\n",
    "                    # Add the answer\n",
    "                    # Score is the sum of logits for the start and end position of the answer\n",
    "                    # Include the text which is taken directly from the context\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char:end_char],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # If we have valid answers, choose the best one\n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction,\n",
    "            # we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "\n",
    "        # Choose the best answer as the prediction for the current example\n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def preprocess_train(dataset):\n",
    "    \"\"\"Preprocess the training dataset\"\"\"\n",
    "    # Some of the questions have lots of whitespace on the left,\n",
    "    # which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space).\n",
    "    # So we remove that\n",
    "    # left whitespace\n",
    "    dataset[\"question\"] = [q.lstrip() for q in dataset[\"question\"]]\n",
    "\n",
    "    # Tokenize our dataset with truncation and padding,\n",
    "    # but keep the overflows using a stride. This results\n",
    "    # in one example possibly giving several features when a context is long,\n",
    "    # each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature, the stride being the number\n",
    "    # of overlapping tokens in the overlap.\n",
    "    tokenized_dataset = TOKENIZER(\n",
    "        dataset[\"question\"],\n",
    "        dataset[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context,\n",
    "    # we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_dataset.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to\n",
    "    # character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_dataset.pop(\"offset_mapping\")\n",
    "\n",
    "    # Store start and end character positions for answers in context\n",
    "    tokenized_dataset[\"start_positions\"] = []\n",
    "    tokenized_dataset[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_dataset[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(TOKENIZER.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example\n",
    "        # (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_dataset.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this\n",
    "        # is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = dataset[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_dataset[\"start_positions\"].append(cls_index)\n",
    "            tokenized_dataset[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span\n",
    "            # (in which case this feature is labeled with the CLS index).\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_dataset[\"start_positions\"].append(cls_index)\n",
    "                tokenized_dataset[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and\n",
    "                # token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset\n",
    "                # if the answer is the last word (edge case).\n",
    "                while (\n",
    "                    token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_dataset[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_dataset[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "def create_datasets():\n",
    "    \"\"\"Load the SQuAD dataset, the tokenized version, and the validation set\"\"\"\n",
    "    squad = load_dataset(\"squad\")\n",
    "\n",
    "    # Preprocessing changes number of samples, so we need to remove some columns so\n",
    "    # the data updates properly\n",
    "    tokenized_data = squad.map(\n",
    "        preprocess_train, batched=True, remove_columns=squad[\"train\"].column_names\n",
    "    )\n",
    "    eval_data = squad[\"validation\"].map(\n",
    "        preprocess_validation, batched=True, remove_columns=squad[\"validation\"].column_names\n",
    "    )\n",
    "\n",
    "    return squad, tokenized_data, eval_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edcc961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference(model, trainer, squad, eval_data, writer, max_inference_time=1e6, n_times=9):\n",
    "    \"\"\"Perform inference experiment at weight noise level specified at runtime.\n",
    "    SQuAD exact match and f1 metrics are captured in Tensorboard\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper functions\n",
    "    def predict():\n",
    "        # Perform inference + evaluate metric here\n",
    "        raw_predictions = trainer.predict(eval_data)\n",
    "        predictions = postprocess_predictions(\n",
    "            squad[\"validation\"], eval_data, raw_predictions.predictions\n",
    "        )\n",
    "\n",
    "        # Format to list of dicts instead of a large dict\n",
    "        formatted_preds = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "        out_metric = metric.compute(predictions=formatted_preds, references=ground_truth)\n",
    "\n",
    "        return out_metric[\"f1\"], out_metric[\"exact_match\"]\n",
    "\n",
    "    def write_metrics(f1, exact_match, t_inference):\n",
    "        # Add information to tensorboard\n",
    "        writer.add_scalar(\"val/f1\", f1, t_inference)\n",
    "        writer.add_scalar(\"val/exact_match\", exact_match, t_inference)\n",
    "\n",
    "    print(\"INFERENCE\")\n",
    "    metric = load(\"squad\")\n",
    "\n",
    "    ground_truth = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in squad[\"validation\"]]\n",
    "    #f1, exact_match = predict()\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    t_inference_list = [1, 60*10, 3600, 3600 * 24, 3600 * 24*7, 3600 * 24 *30, 3600 * 24 *365, 3600 * 24 *365*2, 3600 * 24 *365*5, 3600 * 24 * 365 * 10] \n",
    "    f1_time = []\n",
    "    # Get the initial metrics\n",
    "    #f1, exact_match = predict()\n",
    "    #write_metrics(f1, exact_match, 0.0)\n",
    "\n",
    "    for t_inference in t_inference_list:\n",
    "        print(t_inference)\n",
    "        model.drift_analog_weights(t_inference)\n",
    "        f1, exact_match = predict()\n",
    "        f1_time.append(f1)\n",
    "        print(f\"Exact match: {exact_match: .2f}\\t\" f\"F1: {f1: .2f}\\t\" f\"Drift: {t_inference: .2e}\")\n",
    "    return f1_time \n",
    "        #write_metrics(f1, exact_match, t_inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59afc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainer(model, optimizer, tokenized_data):\n",
    "    \"\"\"Create the Huggingface Trainer\"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./\",\n",
    "        save_strategy=\"no\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=256,\n",
    "        dataloader_num_workers=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.001,\n",
    "        no_cuda=False,\n",
    "    )\n",
    "\n",
    "    collator = DefaultDataCollator()\n",
    "\n",
    "    log_dir = \"logs/fit/25_JUN\" \n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collator,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"validation\"],\n",
    "        tokenizer=TOKENIZER,\n",
    "        optimizers=(optimizer, None),\n",
    "        callbacks=[TensorBoardCallback(writer)],\n",
    "    )\n",
    "\n",
    "    return trainer, writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "squad, tokenized_data, eval_data = create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75f901fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at csarron/bert-base-uncased-squad-v1 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "rpu_config = create_rpu_config(modifier_noise=0.0)\n",
    "model = create_model(rpu_config)\n",
    "optimizer = AnalogSGD(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bef97f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3687053/1214499550.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "You are adding a <class 'transformers.integrations.integration_utils.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n"
     ]
    }
   ],
   "source": [
    "trainer, writer = make_trainer(model, optimizer, tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd792380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8db3bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFERENCE\n",
      "1\n",
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  0.30\tF1:  8.65\tDrift:  1.00e+00\n",
      "600\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  11.78\tF1:  31.27\tDrift:  6.00e+02\n",
      "3600\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  20.71\tF1:  39.88\tDrift:  3.60e+03\n",
      "86400\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  74.84\tF1:  84.59\tDrift:  8.64e+04\n",
      "604800\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  79.92\tF1:  87.66\tDrift:  6.05e+05\n",
      "2592000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  78.20\tF1:  86.26\tDrift:  2.59e+06\n",
      "31536000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  68.39\tF1:  79.79\tDrift:  3.15e+07\n",
      "63072000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  32.29\tF1:  50.31\tDrift:  6.31e+07\n",
      "157680000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  19.29\tF1:  36.66\tDrift:  1.58e+08\n",
      "315360000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 10570 example predictions split into 11033 features.\n",
      "Exact match:  4.76\tF1:  11.94\tDrift:  3.15e+08\n"
     ]
    }
   ],
   "source": [
    "f1_times = do_inference(model, trainer, squad, eval_data,writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpu_config = create_rpu_config(modifier_noise=0.1)\n",
    "squad, tokenized_data, eval_data = create_datasets()\n",
    "model = create_model(rpu_config)\n",
    "optimizer = AnalogSGD(model.parameters(), lr=1e-3)\n",
    "trainer, writer = make_trainer(model, optimizer, tokenized_data)\n",
    "do_inference(model, trainer, squad, eval_data,writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c447cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_rpu_config(noise_model=None):\n",
    "    input_prec = 6\n",
    "    output_prec = 8\n",
    "    my_rpu_config = InferenceRPUConfig()\n",
    "    my_rpu_config.mapping.digital_bias = True # do the bias of the MVM digitally\n",
    "    #my_rpu_config.mapping.max_input_size = 256\n",
    "    #my_rpu_config.mapping.max_output_size = 256\n",
    "    my_rpu_config.forward = IOParametersIRDropT()\n",
    "    my_rpu_config.noise_model = noise_model\n",
    "    my_rpu_config.drift_compensation = None    #my_rpu_config.noise_model = PCMLikeNoiseModel(g_max=25.0)\n",
    "    #my_rpu_config.drift_compensation = GlobalDriftCompensation()\n",
    "    my_rpu_config.forward.ir_drop_g_ratio = 1.0 / 0.35 / (noise_model.g_max*1e-6) # change to 25w-6 when using PCM\n",
    "\n",
    "    #my_rpu_config.drift_compensation = None\n",
    "    my_rpu_config.modifier.std_dev = 0.06\n",
    "    my_rpu_config.modifier.type = WeightModifierType.ADD_NORMAL\n",
    "    \n",
    "    my_rpu_config.forward.inp_res = 1 / (2**input_prec - 2)\n",
    "    my_rpu_config.forward.out_res = 1 / (2**output_prec - 2)\n",
    "    my_rpu_config.forward.is_perfect = True\n",
    "    #my_rpu_config.forward.out_noise = 0.0 # Output on the current addition (?)\n",
    "    my_rpu_config.forward.ir_drop = 1.0 # TODO set to 1.0 when activating IR drop effects\n",
    "    my_rpu_config.forward.ir_drop_rs = 0.35 # Default: 0.15\n",
    "    my_rpu_config.pre_post.input_range.enable = True\n",
    "    \n",
    "    #my_rpu_config.pre_post.input_range.manage_output_clipping = True\n",
    "    my_rpu_config.pre_post.input_range.decay = 0.001\n",
    "    my_rpu_config.pre_post.input_range.input_min_percentage = 0.95\n",
    "    my_rpu_config.pre_post.input_range.output_min_percentage = 0.95\n",
    "    #my_rpu_config.forward.noise_management = NoiseManagementType.ABS_MAX # Rescale back the output with the scaling for normalizing the input\n",
    "    my_rpu_config.forward.bound_management = BoundManagementType.ITERATIVE\n",
    "    my_rpu_config.clip.type = WeightClipType.LAYER_GAUSSIAN\n",
    "    my_rpu_config.clip.sigma = 2.5\n",
    "    my_rpu_config.forward.out_bound = 10.0  # quite restric\n",
    "    return my_rpu_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_max = 90\n",
    "g_min = 10\n",
    "prog_overshoot =1.23\n",
    "single_device = True\n",
    "acceptance_range = 0.2\n",
    "reram_noise =ReRamCMONoiseModel(g_max=g_max, g_min=g_min,\n",
    "                                                    acceptance_range=acceptance_range,\n",
    "                                                    resistor_compensator=prog_overshoot,\n",
    "                                                    single_device=single_device)\n",
    "rpu_config= gen_rpu_config(reram_noise)\n",
    "a_model = convert_to_analog(model, rpu_config=rpu_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_model.load_state_dict(torch.load(SAVE_PATH, map_location='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rep = 5\n",
    "t_inferences = [1, 60*10, 3600, 3600 * 24, 3600 * 24*7, 3600 * 24 *30, 3600 * 24 *365, 3600 * 24 *365*2, 3600 * 24 *365*5, 3600 * 24 * 365 * 10]\n",
    "drifted_test_accs = torch.zeros(size=(len(t_inferences),n_rep))\n",
    "a_model.eval()\n",
    "for i,t in enumerate(t_inferences):\n",
    "    for j in range(n_rep):\n",
    "        a_model.drift_analog_weights(t)\n",
    "        print(\"Drifted at t: \", t)\n",
    "        _,_,err, accuracy = test_evaluation(validation_data=validation_data, model=a_model, criterion=criterion)\n",
    "        drifted_test_accs[i, j] = accuracy\n",
    "        print(f\"Accuracy of the analog model: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT on SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab611a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(drifted_test_accs, \"/u/mvc/aihwkit/drift_compensation_NNs/hwa_vgg8_1T1R_10-90_baseline.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6cc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41704c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
