{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22eb9d0",
   "metadata": {},
   "source": [
    "# VGG8 Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6b79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Imports from PyTorch.\n",
    "from torch import nn, Tensor, device, no_grad, manual_seed\n",
    "from torch import max as torch_max\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Imports from aihwkit.\n",
    "from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n",
    "import torch \n",
    "from aihwkit.optim import AnalogSGD\n",
    "from aihwkit.simulator.configs import MappingParameter\n",
    "from aihwkit.simulator.rpu_base import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb05ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = 0\n",
    "if cuda.is_compiled():\n",
    "    USE_CUDA = 1\n",
    "DEVICE = device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "# Path to store datasets\n",
    "PATH_DATASET = os.path.join(\"data\", \"DATASET\")\n",
    "\n",
    "# Path to store results\n",
    "RESULTS = os.path.join(os.getcwd(), \"results\", \"VGG8\")\n",
    "\n",
    "SAVE_PATH = \"/u/mvc/aihwkit/notebooks/tutorial/Models/hwa_vgg8.th\"\n",
    "# Training parameters\n",
    "SEED = 1\n",
    "N_EPOCHS = 80\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.1\n",
    "N_CLASSES = 10\n",
    "WEIGHT_SCALING_OMEGA = 0.6  # Should not be larger than max weight.\n",
    "\n",
    "# Select the device model to use in the training. In this case we are using one of the preset,\n",
    "# but it can be changed to a number of preset to explore possible different analog devices\n",
    "mapping = MappingParameter(weight_scaling_omega=WEIGHT_SCALING_OMEGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images():\n",
    "    \"\"\"Load images for train from torchvision datasets.\"\"\"\n",
    "    mean = Tensor([0.4377, 0.4438, 0.4728])\n",
    "    std = Tensor([0.1980, 0.2010, 0.1970])\n",
    "\n",
    "    print(f\"Normalization data: ({mean},{std})\")\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    train_set = datasets.SVHN(PATH_DATASET, download=True, split=\"train\", transform=transform)\n",
    "    val_set = datasets.SVHN(PATH_DATASET, download=True, split=\"test\", transform=transform)\n",
    "    train_data = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validation_data = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return train_data, validation_data\n",
    "\n",
    "\n",
    "def create_digital_network():\n",
    "    \"\"\"Create a Vgg8 inspired analog model.\n",
    "\n",
    "    Returns:\n",
    "       nn.Module: VGG8 model\n",
    "    \"\"\"\n",
    "    channel_base = 48\n",
    "    channel = [channel_base, 2 * channel_base, 3 * channel_base]\n",
    "    fc_size = 8 * channel_base\n",
    "    model = torch.nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=channel[0], kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(\n",
    "            in_channels=channel[0],\n",
    "            out_channels=channel[0],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        ),\n",
    "        nn.BatchNorm2d(channel[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n",
    "        nn.Conv2d(\n",
    "            in_channels=channel[0],\n",
    "            out_channels=channel[1],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        ),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(\n",
    "            in_channels=channel[1],\n",
    "            out_channels=channel[1],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        ),\n",
    "        nn.BatchNorm2d(channel[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n",
    "        nn.Conv2d(\n",
    "            in_channels=channel[1],\n",
    "            out_channels=channel[2],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        ),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(\n",
    "            in_channels=channel[2],\n",
    "            out_channels=channel[2],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        ),\n",
    "        nn.BatchNorm2d(channel[2]),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=16 * channel[2], out_features=fc_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=fc_size, out_features=N_CLASSES),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_analog_network():\n",
    "    \"\"\"Create a Vgg8 inspired analog model.\n",
    "\n",
    "    Returns:\n",
    "       nn.Module: VGG8 model\n",
    "    \"\"\"\n",
    "    channel_base = 48\n",
    "    channel = [channel_base, 2 * channel_base, 3 * channel_base]\n",
    "    fc_size = 8 * channel_base\n",
    "    model = AnalogSequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=channel[0], kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        AnalogConv2d(\n",
    "            in_channels=channel[0],\n",
    "            out_channels=channel[0],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            rpu_config=RPU_CONFIG,\n",
    "        ),\n",
    "        nn.BatchNorm2d(channel[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n",
    "        AnalogConv2d(\n",
    "            in_channels=channel[0],\n",
    "            out_channels=channel[1],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            rpu_config=RPU_CONFIG,\n",
    "        ),\n",
    "        nn.ReLU(),\n",
    "        AnalogConv2d(\n",
    "            in_channels=channel[1],\n",
    "            out_channels=channel[1],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            rpu_config=RPU_CONFIG,\n",
    "        ),\n",
    "        nn.BatchNorm2d(channel[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n",
    "        AnalogConv2d(\n",
    "            in_channels=channel[1],\n",
    "            out_channels=channel[2],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            rpu_config=RPU_CONFIG,\n",
    "        ),\n",
    "        nn.ReLU(),\n",
    "        AnalogConv2d(\n",
    "            in_channels=channel[2],\n",
    "            out_channels=channel[2],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            rpu_config=RPU_CONFIG,\n",
    "        ),\n",
    "        nn.BatchNorm2d(channel[2]),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1),\n",
    "        nn.Flatten(),\n",
    "        AnalogLinear(in_features=16 * channel[2], out_features=fc_size, rpu_config=RPU_CONFIG),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=fc_size, out_features=N_CLASSES),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51cbf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluation(validation_data, model, criterion):\n",
    "    \"\"\"Test trained network\n",
    "\n",
    "    Args:\n",
    "        validation_data (DataLoader): Validation set to perform the evaluation\n",
    "        model (nn.Module): Trained model to be evaluated\n",
    "        criterion (nn.CrossEntropyLoss): criterion to compute loss\n",
    "\n",
    "    Returns:\n",
    "        nn.Module, float, float, float: model, test epoch loss, test error, and test accuracy\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    predicted_ok = 0\n",
    "    total_images = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for images, labels in validation_data:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        pred = model(images)\n",
    "        loss = criterion(pred, labels)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = torch_max(pred.data, 1)\n",
    "        total_images += labels.size(0)\n",
    "        predicted_ok += (predicted == labels).sum().item()\n",
    "        accuracy = predicted_ok / total_images * 100\n",
    "        error = (1 - predicted_ok / total_images) * 100\n",
    "\n",
    "    epoch_loss = total_loss / len(validation_data.dataset)\n",
    "\n",
    "    return model, epoch_loss, error, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d694131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization data: (tensor([0.4377, 0.4438, 0.4728]),tensor([0.1980, 0.2010, 0.1970]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/DATASET/train_32x32.mat\n",
      "Using downloaded and verified file: data/DATASET/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(RESULTS, exist_ok=True)\n",
    "manual_seed(SEED)\n",
    "from aihwkit.nn.conversion import convert_to_analog\n",
    "from aihwkit.inference import ReRamCMONoiseModel\n",
    "from aihwkit.simulator.parameters.io import IOParametersIRDropT\n",
    "\n",
    "# Load datasets.\n",
    "train_data, validation_data = load_images()\n",
    "\n",
    "# Prepare the model.\n",
    "model = create_digital_network()\n",
    "#model.load_state_dict(torch.load(\"/u/mvc/aihwkit/notebooks/tutorial/Models/pre-trained-vgg8.th\", map_location='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c447cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.configs import (\n",
    "    InferenceRPUConfig,\n",
    "    NoiseManagementType,\n",
    "    BoundManagementType,\n",
    ")\n",
    "from aihwkit.simulator.parameters.io import IOParametersIRDropT\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    WeightModifierType,\n",
    "    BoundManagementType,\n",
    "    WeightClipType,\n",
    "    NoiseManagementType,\n",
    "    WeightRemapType,\n",
    ")\n",
    "def gen_rpu_config(noise_model=None):\n",
    "    input_prec = 6\n",
    "    output_prec = 8\n",
    "    my_rpu_config = InferenceRPUConfig()\n",
    "    my_rpu_config.mapping.digital_bias = True # do the bias of the MVM digitally\n",
    "    #my_rpu_config.mapping.max_input_size = 256\n",
    "    #my_rpu_config.mapping.max_output_size = 256\n",
    "    my_rpu_config.forward = IOParametersIRDropT()\n",
    "    my_rpu_config.noise_model = noise_model\n",
    "    my_rpu_config.drift_compensation = None    #my_rpu_config.noise_model = PCMLikeNoiseModel(g_max=25.0)\n",
    "    #my_rpu_config.drift_compensation = GlobalDriftCompensation()\n",
    "    my_rpu_config.forward.ir_drop_g_ratio = 1.0 / 0.35 / (noise_model.g_max*1e-6) # change to 25w-6 when using PCM\n",
    "\n",
    "    #my_rpu_config.drift_compensation = None\n",
    "    my_rpu_config.modifier.std_dev = 0.06\n",
    "    my_rpu_config.modifier.type = WeightModifierType.ADD_NORMAL\n",
    "    \n",
    "    my_rpu_config.forward.inp_res = 1 / (2**input_prec - 2)\n",
    "    my_rpu_config.forward.out_res = 1 / (2**output_prec - 2)\n",
    "    my_rpu_config.forward.is_perfect = True\n",
    "    #my_rpu_config.forward.out_noise = 0.0 # Output on the current addition (?)\n",
    "    my_rpu_config.forward.ir_drop = 1.0 # TODO set to 1.0 when activating IR drop effects\n",
    "    my_rpu_config.forward.ir_drop_rs = 0.35 # Default: 0.15\n",
    "    my_rpu_config.pre_post.input_range.enable = True\n",
    "    \n",
    "    #my_rpu_config.pre_post.input_range.manage_output_clipping = True\n",
    "    my_rpu_config.pre_post.input_range.decay = 0.001\n",
    "    my_rpu_config.pre_post.input_range.input_min_percentage = 0.95\n",
    "    my_rpu_config.pre_post.input_range.output_min_percentage = 0.95\n",
    "    #my_rpu_config.forward.noise_management = NoiseManagementType.ABS_MAX # Rescale back the output with the scaling for normalizing the input\n",
    "    my_rpu_config.forward.bound_management = BoundManagementType.ITERATIVE\n",
    "    my_rpu_config.clip.type = WeightClipType.LAYER_GAUSSIAN\n",
    "    my_rpu_config.clip.sigma = 2.5\n",
    "    my_rpu_config.forward.out_bound = 10.0  # quite restric\n",
    "    return my_rpu_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba74709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_max = 50\n",
    "g_min = 10\n",
    "prog_overshoot =1.23\n",
    "single_device = False\n",
    "acceptance_range = 0.2\n",
    "reram_noise =ReRamCMONoiseModel(g_max=g_max, g_min=g_min,\n",
    "                                                    acceptance_range=acceptance_range,\n",
    "                                                    resistor_compensator=prog_overshoot,\n",
    "                                                    single_device=single_device)\n",
    "rpu_config= gen_rpu_config(reram_noise)\n",
    "a_model = convert_to_analog(model, rpu_config=rpu_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb4db04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1045349/15031717.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a_model.load_state_dict(torch.load(\"/u/mvc/aihwkit/notebooks/tutorial/Models/hwa_2t2r_vgg8.th\", map_location='cuda'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnalogSequential(\n",
       "  (0): AnalogConv2d(\n",
       "    3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), InferenceRPUConfig\n",
       "    (analog_module): InferenceTile(RPUCudaPulsed<float>[SimpleRPUDevice](48,27))\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): AnalogConv2d(\n",
       "    48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), InferenceRPUConfig\n",
       "    (analog_module): InferenceTile(RPUCudaPulsed<float>[SimpleRPUDevice](48,432))\n",
       "  )\n",
       "  (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): AnalogConv2d(\n",
       "    48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), InferenceRPUConfig\n",
       "    (analog_module): InferenceTile(RPUCudaPulsed<float>[SimpleRPUDevice](96,432))\n",
       "  )\n",
       "  (7): ReLU()\n",
       "  (8): AnalogConv2d(\n",
       "    96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), InferenceRPUConfig\n",
       "    (analog_module): InferenceTile(RPUCudaPulsed<float>[SimpleRPUDevice](96,864))\n",
       "  )\n",
       "  (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (12): AnalogConv2d(\n",
       "    96, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), InferenceRPUConfig\n",
       "    (analog_module): InferenceTile(RPUCudaPulsed<float>[SimpleRPUDevice](144,864))\n",
       "  )\n",
       "  (13): ReLU()\n",
       "  (14): AnalogConv2d(\n",
       "    144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), InferenceRPUConfig\n",
       "    (analog_module): InferenceTile(RPUCudaPulsed<float>[SimpleRPUDevice](144,1296))\n",
       "  )\n",
       "  (15): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): ReLU()\n",
       "  (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (18): Flatten(start_dim=1, end_dim=-1)\n",
       "  (19): AnalogLinear(\n",
       "    in_features=2304, out_features=384, bias=True, InferenceRPUConfig\n",
       "    (analog_module): TileModuleArray(\n",
       "      (array): ModuleList(\n",
       "        (0-3): 4 x ModuleList(\n",
       "          (0): InferenceTile(RPUCudaPulsed<float>[SimpleRPUDevice](384,461))\n",
       "        )\n",
       "        (4): ModuleList(\n",
       "          (0): InferenceTile(RPUCudaPulsed<float>[SimpleRPUDevice](384,460))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (20): ReLU()\n",
       "  (21): AnalogLinear(\n",
       "    in_features=384, out_features=10, bias=True, InferenceRPUConfig\n",
       "    (analog_module): InferenceTile(RPUCudaPulsed<float>[SimpleRPUDevice](10,384))\n",
       "  )\n",
       "  (22): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a_model.load_state_dict(torch.load(SAVE_PATH, map_location='cuda'))\n",
    "a_model.load_state_dict(torch.load(\"/u/mvc/aihwkit/notebooks/tutorial/Models/hwa_2t2r_vgg8.th\", map_location='cuda'))\n",
    "a_model.eval()\n",
    "#a_model.program_analog_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ec14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349b77c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drifted at t:  1\n",
      "Accuracy of the analog model: 94.33%\n",
      "Drifted at t:  1\n",
      "Accuracy of the analog model: 94.34%\n",
      "Drifted at t:  1\n",
      "Accuracy of the analog model: 94.33%\n",
      "Drifted at t:  1\n",
      "Accuracy of the analog model: 94.32%\n",
      "Drifted at t:  1\n",
      "Accuracy of the analog model: 94.33%\n",
      "Drifted at t:  600\n",
      "Accuracy of the analog model: 94.43%\n",
      "Drifted at t:  600\n",
      "Accuracy of the analog model: 94.38%\n",
      "Drifted at t:  600\n",
      "Accuracy of the analog model: 94.41%\n",
      "Drifted at t:  600\n",
      "Accuracy of the analog model: 94.51%\n",
      "Drifted at t:  600\n",
      "Accuracy of the analog model: 94.53%\n",
      "Drifted at t:  3600\n",
      "Accuracy of the analog model: 94.48%\n",
      "Drifted at t:  3600\n",
      "Accuracy of the analog model: 94.48%\n",
      "Drifted at t:  3600\n",
      "Accuracy of the analog model: 94.43%\n",
      "Drifted at t:  3600\n",
      "Accuracy of the analog model: 94.51%\n",
      "Drifted at t:  3600\n",
      "Accuracy of the analog model: 94.45%\n",
      "Drifted at t:  86400\n",
      "Accuracy of the analog model: 94.54%\n",
      "Drifted at t:  86400\n",
      "Accuracy of the analog model: 94.56%\n",
      "Drifted at t:  86400\n",
      "Accuracy of the analog model: 94.47%\n",
      "Drifted at t:  86400\n",
      "Accuracy of the analog model: 94.58%\n",
      "Drifted at t:  86400\n",
      "Accuracy of the analog model: 94.46%\n",
      "Drifted at t:  604800\n",
      "Accuracy of the analog model: 94.46%\n",
      "Drifted at t:  604800\n",
      "Accuracy of the analog model: 94.58%\n",
      "Drifted at t:  604800\n",
      "Accuracy of the analog model: 94.50%\n",
      "Drifted at t:  604800\n",
      "Accuracy of the analog model: 94.46%\n",
      "Drifted at t:  604800\n",
      "Accuracy of the analog model: 94.52%\n",
      "Drifted at t:  2592000\n",
      "Accuracy of the analog model: 94.50%\n",
      "Drifted at t:  2592000\n",
      "Accuracy of the analog model: 94.52%\n",
      "Drifted at t:  2592000\n",
      "Accuracy of the analog model: 94.48%\n",
      "Drifted at t:  2592000\n",
      "Accuracy of the analog model: 94.51%\n",
      "Drifted at t:  2592000\n",
      "Accuracy of the analog model: 94.45%\n",
      "Drifted at t:  31536000\n",
      "Accuracy of the analog model: 94.48%\n",
      "Drifted at t:  31536000\n",
      "Accuracy of the analog model: 94.48%\n",
      "Drifted at t:  31536000\n",
      "Accuracy of the analog model: 94.48%\n",
      "Drifted at t:  31536000\n",
      "Accuracy of the analog model: 94.50%\n",
      "Drifted at t:  31536000\n",
      "Accuracy of the analog model: 94.31%\n",
      "Drifted at t:  63072000\n",
      "Accuracy of the analog model: 94.44%\n",
      "Drifted at t:  63072000\n",
      "Accuracy of the analog model: 94.43%\n",
      "Drifted at t:  63072000\n",
      "Accuracy of the analog model: 94.35%\n",
      "Drifted at t:  63072000\n",
      "Accuracy of the analog model: 94.45%\n",
      "Drifted at t:  63072000\n",
      "Accuracy of the analog model: 94.53%\n",
      "Drifted at t:  157680000\n",
      "Accuracy of the analog model: 94.54%\n",
      "Drifted at t:  157680000\n",
      "Accuracy of the analog model: 94.42%\n",
      "Drifted at t:  157680000\n",
      "Accuracy of the analog model: 94.49%\n",
      "Drifted at t:  157680000\n",
      "Accuracy of the analog model: 94.42%\n",
      "Drifted at t:  157680000\n",
      "Accuracy of the analog model: 94.56%\n",
      "Drifted at t:  315360000\n",
      "Accuracy of the analog model: 94.44%\n",
      "Drifted at t:  315360000\n",
      "Accuracy of the analog model: 94.22%\n",
      "Drifted at t:  315360000\n",
      "Accuracy of the analog model: 94.55%\n",
      "Drifted at t:  315360000\n",
      "Accuracy of the analog model: 94.41%\n",
      "Drifted at t:  315360000\n",
      "Accuracy of the analog model: 94.48%\n"
     ]
    }
   ],
   "source": [
    "n_rep = 5\n",
    "t_inferences = [1, 60*10, 3600, 3600 * 24, 3600 * 24*7, 3600 * 24 *30, 3600 * 24 *365, 3600 * 24 *365*2, 3600 * 24 *365*5, 3600 * 24 * 365 * 10]\n",
    "#t_inferences = [ 3600 * 24]\n",
    "drifted_test_accs = torch.zeros(size=(len(t_inferences),n_rep))\n",
    "\n",
    "for i,t in enumerate(t_inferences):\n",
    "    for j in range(n_rep):\n",
    "        a_model.drift_analog_weights(t)\n",
    "        print(\"Drifted at t: \", t)\n",
    "        _,_,err, accuracy = test_evaluation(validation_data=validation_data, model=a_model, criterion=criterion)\n",
    "        drifted_test_accs[i, j] = accuracy\n",
    "        print(f\"Accuracy of the analog model: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a771f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT on SQUAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ab611a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(drifted_test_accs, \"/u/mvc/aihwkit/drift_compensation_NNs/hwa_vgg8_2T2R_10-50_prog.th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6cc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41704c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
